{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eRiRgm4P5LIL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import SnowballStemmer # Stemming\n",
        "from nltk.tokenize import RegexpTokenizer # Tokenizing\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.cluster import KMeans # K-Means Clusterting\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.cluster.hierarchy import ward, dendrogram, linkage, fcluster # Ward Clustering\n",
        "from scipy.spatial import distance\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "def tokenize_and_stem(text):\n",
        "\n",
        "  text = str(text)\n",
        "  text1 = text.lower()\n",
        "  text1 = text1.replace('{html}', \"\") # Remove weblinks\n",
        "  text1 = text1.replace('/p[', \"\")\n",
        "  cleantext = re.sub(r'[^\\w\\s]', text1)\n",
        "  rem_num = re.sub('[0-9]+', cleantext)\n",
        "  tokens = tokenizer.tokenize(rem_num)\n",
        "  stem_words = [stemmer.stem(w) for w in tokens]\n",
        "\n",
        "  return stem_words"
      ],
      "metadata": {
        "id": "SRrw9A8b67TG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71iKcmTM8vvh",
        "outputId": "ae550dee-f6e3-4e55-d571-119cbf7c623e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Understanding TF-IDF:\n",
        "###https://www.geeksforgeeks.org/understanding-tf-idf-term-frequency-inverse-document-frequency/"
      ],
      "metadata": {
        "id": "ZYheC5CO9nKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tfidf_calc(comments, ngram, stopwords):\n",
        "\n",
        "  tfidf_vectorizer = TfidfVectorizer(max_df = 0.5, max_features = 1000000, min_df = 0.05, use_idf = True, stop_words = stopwords, ngram_ranges = (1, 4))\n",
        "  tfidf_matrix = tfidf_vectorizer.fit_transform(comments) # Fit vectorizer to complaints\n",
        "  terms = tfidf_vectorizer.get_feature_names() # List of features used in the tf-idf matrix\n",
        "\n",
        "  return tfidf_matrix, terms"
      ],
      "metadata": {
        "id": "kDC9qfvZ8bHr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Difference between K-means and Hierarchial clustering:\n",
        "###https://www.geeksforgeeks.org/difference-between-k-means-and-hierarchical-clustering/"
      ],
      "metadata": {
        "id": "oLGrts4WAj2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kmeans(tfidf_matrix, num_clusters):\n",
        "\n",
        "  km = KMeans(n_clusters = num_clusters).fit(tfidf_matrix)\n",
        "  clusters = km.labels_\n",
        "  cluster_centers = km.cluster_centers\n",
        "  return clusters, cluster_centers\n",
        "\n",
        "def wardClustering(tfidf_matrix, max_d):\n",
        "\n",
        "  dist = 1-cosine_similarity(tfidf_matrix)\n",
        "  dist = np.clip(dist, 0, 1)\n",
        "  dist_cnds = distance.squareform(dist, checks = True)\n",
        "\n",
        "  linkage_matrix = ward(dist_cnds)\n",
        "  clusters = fcluster(linkage_matrix, max_d, criterion = 'distance')\n",
        "  clusters_df = pd.DataFrame(clusters, columns = ['cluster']).reset_index()\n",
        "  cluster_centers = np.asarray(np.vstack(clusters_df.groupby('cluster').apply(lambda x: np.mean(tfidf_matrix[x['index'].tolist(), :], axis= 0))))\n",
        "\n",
        "  return clusters, cluster_centers, linkage_matrix"
      ],
      "metadata": {
        "id": "ha03lmv5-lLh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def topNwords(clusters, cluster_centers, terms, n):\n",
        "  order_centroids = cluster_centers.arg_sort()[:, ::-1]\n",
        "  topN_ind = order_centroids[:, :n]\n",
        "  words_matrix = np.array(terms[i] for j in topN_ind for i in j).reshape(topN_ind.shape)\n",
        "\n",
        "  topNwords_df = pd.DataFrame(words_matrix, columns = ['top' + str(i) for i in range(1, n+1)])\n",
        "  _, topNwords_df['size'] = np.unique(clusters, return_counts = True)\n",
        "\n",
        "  return topNwords_df"
      ],
      "metadata": {
        "id": "WmXyG65DA2bD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(df, n_cluster, n_words, max_d, method):\n",
        "\n",
        "  cluster_centers = np.zeros((5,10))\n",
        "\n",
        "  tfidf_matrix, terms = tfidf_calc(finaldf['Consumer complaint narrative1'], (1, 3), stopwords)\n",
        "\n",
        "  if method == 'kmeans':\n",
        "    cluster, cluster_centers = kmeans(tfidf_matrix, n_cluster)\n",
        "  elif method == 'ward':\n",
        "    clusters, cluster_centers, linkage_matrix = wardClustering(tfidf_matrix, max_d)\n",
        "\n",
        "  topNwords_df = topNwords(clusters, cluster_centers, terms, n_words)\n",
        "  finaldf['cluster'] = pd.Series(clusters, index = df.index)\n",
        "\n",
        "  if method == 'kmeans':\n",
        "    return topNwords_df, df\n",
        "  elif method == 'ward':\n",
        "    return topNwords_df, df, linkage_matrix"
      ],
      "metadata": {
        "id": "R4ewVI3HB8he"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def array2set(textArray):\n",
        "\n",
        "  word_list = []\n",
        "  for i in textArray:\n",
        "    word_list.extend(i.split(' '))\n",
        "\n",
        "  return set(word_list)"
      ],
      "metadata": {
        "id": "rbyB8VtwELGN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def alert(topNwords_df, df_week, threshold4vs3, threshold4vs2, threshold4vs1, floor, min_size, max_d, excludeTopics):\n",
        "\n",
        "  cluster_countA = df_week.groupby(['cluster']).apply(lambda x: x.shape[0]).reset_index(name = 'volume')\n",
        "  cluster_countB = df_week.groupby(['cluster', 'month']).apply(lambda x: x.shape[0]).reset_index(name = 'volume')\n",
        "\n",
        "  cluster_count = cluster_countB.pivot(index = 'cluster', columns = 'month', values = 'volume')\n",
        "\n",
        "  i = max(df_week['month'])\n",
        "  j = i - 1\n",
        "  q = min(df_week['month'])\n",
        "  k = q+1\n",
        "\n",
        "  week_rate1 = ((cluster_count[i]/cluster_count[i].sum())/(cluster_count[j]/cluster_count[j].sum()))\n",
        "  week_rate2 = ((cluster_count[i]/cluster_count[i].sum())/(cluster_count[k]/cluster_count[k].sum()))\n",
        "  week_rate3 = ((cluster_count[i]/cluster_count[i].sum())/(cluster_count[q]/cluster_count[q].sum()))\n",
        "\n",
        "  topNwords_df['rate4vs3'] = week_rate1.values\n",
        "  topNwords_df['rate4vs2'] = week_rate2.values\n",
        "  topNwords_df['rate4vs1'] = week_rate3.values\n",
        "\n",
        "  alert_topN = topNwords_df.loc[((cluster_countA['volume'] > floor) &\n",
        "   ((topNwords_df['rate4vs3'] > threshold4vs3) | (topNwords_df['rate4vs2'] > threshold4vs2) | (topNwords_df['rate4vs1'] > threshold4vs1)\n",
        "   | (cluster_countA['volume'] > min_size)))]\n",
        "\n",
        "  index_lst = list(filter(lambda x: len(array2set(alert_topicN.iloc[x, :5].values).intersection(set(excludeTopics))) < 2, np.arrange(alert_topN.shape[0])))\n",
        "  alert_topN = alert_topN.iloc[index_lst, :]\n",
        "  alerts_details = df_week.loc[df_week['cluster'].isin(alert_topN['cluster'])]\n",
        "\n",
        "  return alert_topN, alert_details"
      ],
      "metadata": {
        "id": "QwOCQdPCEYw9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('june22.csv', encoding = 'utf-8')\n",
        "finaldf = df.dropna(subset = ['Consumer complaint narrative'])\n",
        "finaldf.shape"
      ],
      "metadata": {
        "id": "evf_AUbdIE1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold4vs3 = 1.2\n",
        "threshold4vs2 = 1.4\n",
        "threshold4vs1 = 1.6\n",
        "\n",
        "max_d = 1.8\n",
        "\n",
        "excludeTopics = ['scra', 'militari']"
      ],
      "metadata": {
        "id": "s_6-DPhWILcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finaldf['Consumer complaint narrative1'] = finaldf['Consumer complaint narrative'].map(lambda s: tokenize_and_stem(s))\n",
        "\n",
        "topNwords_df, df_week, linkage_matrix = main(finaldf, n_cluster = None, n_words = 5, max_d = max_d, method = 'Ward')"
      ],
      "metadata": {
        "id": "2rV-lpglIaD2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}